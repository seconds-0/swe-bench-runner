#!/usr/bin/env python3
"""Verify integration test setup for SWE-bench runner.

This script checks:
1. All integration test files exist
2. Pytest can discover the tests  
3. Tests can run in dry-run mode (will skip due to missing credentials)
4. Reports the overall status
"""

import os
import sys
import subprocess
from pathlib import Path
from typing import List, Tuple, Dict
import json


class IntegrationTestVerifier:
    """Verifies integration test setup and structure."""
    
    def __init__(self):
        self.project_root = Path(__file__).parent.parent
        self.integration_dir = self.project_root / "tests" / "integration"
        self.expected_files = [
            "conftest.py",
            "test_openai_integration.py", 
            "test_anthropic_integration.py",
            "test_ollama_integration.py",
            "README.md"
        ]
        self.results: Dict[str, bool] = {}
        self.messages: List[str] = []
        
    def check_files_exist(self) -> bool:
        """Check that all expected integration test files exist."""
        print("\nðŸ” Checking integration test files...")
        all_exist = True
        
        if not self.integration_dir.exists():
            self.messages.append(f"âŒ Integration test directory not found: {self.integration_dir}")
            self.results["files_exist"] = False
            return False
            
        # Check expected files
        for filename in self.expected_files:
            filepath = self.integration_dir / filename
            if filepath.exists():
                print(f"  âœ… Found: {filename}")
            else:
                print(f"  âŒ Missing: {filename}")
                self.messages.append(f"Missing integration test file: {filename}")
                all_exist = False
        
        # Also list any additional test files
        test_files = list(self.integration_dir.glob("test_*.py"))
        additional_files = [f.name for f in test_files if f.name not in self.expected_files]
        if additional_files:
            print("\n  ðŸ“ Additional test files found:")
            for filename in additional_files:
                print(f"    â€¢ {filename}")
                
        self.results["files_exist"] = all_exist
        return all_exist
        
    def check_test_discovery(self) -> bool:
        """Check that pytest can discover the integration tests."""
        print("\nðŸ” Checking pytest test discovery...")
        
        try:
            # Run pytest in collect-only mode to discover tests
            result = subprocess.run(
                [sys.executable, "-m", "pytest", "--collect-only", "-q", str(self.integration_dir)],
                capture_output=True,
                text=True,
                cwd=self.project_root
            )
            
            # Check for import errors
            if "ImportError" in result.stderr or "ModuleNotFoundError" in result.stderr:
                print("  âš ï¸  Some tests could not be collected due to missing dependencies")
                print("  This is expected if optional provider packages are not installed")
            
            # Parse output to count discovered tests
            lines = result.stdout.strip().split('\n')
            test_count = 0
            discovered_tests = []
            errors = []
            
            for line in lines:
                if "tests/integration" in line and "::" in line and "TestCase" not in line:
                    test_count += 1
                    # Extract test name
                    test_name = line.strip()
                    discovered_tests.append(test_name)
                elif "ERROR" in line:
                    errors.append(line.strip())
                    
            if errors:
                print("\n  âš ï¸  Collection errors (expected if provider packages missing):")
                for error in errors[:3]:  # Show first 3 errors
                    print(f"    â€¢ {error}")
                    
            if test_count > 0:
                print(f"\n  âœ… Discovered {test_count} integration tests")
                print("\n  ðŸ“‹ Test classes found:")
                
                # Group by test file
                test_files = {}
                for test in discovered_tests:
                    parts = test.split("::")
                    if len(parts) >= 2:
                        filename = parts[0].split("/")[-1]
                        if filename not in test_files:
                            test_files[filename] = []
                        test_files[filename].append("::".join(parts[1:]))
                        
                for filename, tests in sorted(test_files.items()):
                    print(f"    â€¢ {filename}: {len(tests)} tests")
                    
                self.results["test_discovery"] = True
                return True
            else:
                # Check if we at least found the test files
                if "conftest.py" in result.stdout:
                    print("  âš ï¸  Test files found but no tests collected (likely due to missing dependencies)")
                    self.results["test_discovery"] = True  # Partial success
                    return True
                else:
                    self.messages.append("No integration tests discovered by pytest")
                    self.results["test_discovery"] = False
                    return False
                
        except Exception as e:
            self.messages.append(f"Error during test discovery: {e}")
            self.results["test_discovery"] = False
            return False
            
    def check_dry_run(self) -> bool:
        """Run tests in dry-run mode to check they skip properly without credentials."""
        print("\nðŸ” Running integration tests in dry-run mode...")
        print("  (Tests should skip due to missing credentials - this is expected)")
        
        try:
            # Run pytest with integration marker
            env = os.environ.copy()
            # Ensure no API keys are set for dry run
            env.pop("OPENAI_API_KEY", None)
            env.pop("ANTHROPIC_API_KEY", None)
            
            result = subprocess.run(
                [
                    sys.executable, "-m", "pytest", 
                    "-v",
                    "-m", "integration",
                    "--tb=short",
                    str(self.integration_dir)
                ],
                capture_output=True,
                text=True,
                cwd=self.project_root,
                env=env
            )
            
            # Check output for expected skip messages
            output = result.stdout + result.stderr
            
            # Check for import errors first
            if "ImportError" in output or "ModuleNotFoundError" in output:
                print("\n  âš ï¸  Some tests could not run due to missing provider packages")
                print("  This is expected - integration tests require optional dependencies")
                
                # Check which providers are missing
                missing_providers = []
                if "No module named 'openai'" in output:
                    missing_providers.append("openai")
                if "No module named 'anthropic'" in output:
                    missing_providers.append("anthropic")
                    
                if missing_providers:
                    print(f"\n  â„¹ï¸  To run all integration tests, install: pip install {' '.join(missing_providers)}")
                
                # This is still a success - the test infrastructure works
                self.results["dry_run"] = True
                return True
            
            # Count test results
            skipped_count = output.count("SKIPPED")
            passed_count = output.count("PASSED")
            failed_count = output.count("FAILED")
            error_count = output.count("ERROR")
            
            print(f"\n  ðŸ“Š Test Results:")
            print(f"    â€¢ Skipped: {skipped_count} (expected due to missing credentials)")
            print(f"    â€¢ Passed: {passed_count}")
            print(f"    â€¢ Failed: {failed_count}")
            print(f"    â€¢ Errors: {error_count}")
            
            # Check for expected skip reasons
            skip_reasons = []
            if "OPENAI_API_KEY not set" in output:
                skip_reasons.append("OpenAI API key check âœ…")
            if "ANTHROPIC_API_KEY not set" in output:
                skip_reasons.append("Anthropic API key check âœ…")
            if "Ollama not running" in output:
                skip_reasons.append("Ollama availability check âœ…")
                
            if skip_reasons:
                print("\n  âœ… Skip mechanisms working correctly:")
                for reason in skip_reasons:
                    print(f"    â€¢ {reason}")
                    
            # Success if we have skipped tests or import errors (expected) and no failures
            if (skipped_count > 0 or error_count > 0) and failed_count == 0:
                self.results["dry_run"] = True
                return True
            elif failed_count > 0:
                self.messages.append(f"Some tests failed during dry run: {failed_count} failures")
                self.results["dry_run"] = False
                return False
            else:
                # No tests ran at all - could be due to missing packages
                print("\n  â„¹ï¸  No tests were executed - likely all providers missing")
                self.results["dry_run"] = True  # This is acceptable
                return True
                
        except Exception as e:
            self.messages.append(f"Error during dry run: {e}")
            self.results["dry_run"] = False
            return False
            
    def check_dependencies(self) -> bool:
        """Check that required dependencies are installed."""
        print("\nðŸ” Checking required dependencies...")
        
        required_packages = [
            ("pytest", "pytest"),
            ("aiohttp", "aiohttp"),
        ]
        
        optional_packages = [
            ("openai", "openai"),
            ("anthropic", "anthropic"),
        ]
        
        all_required_installed = True
        
        # Check required packages
        print("  Required packages:")
        for import_name, package_name in required_packages:
            try:
                __import__(import_name)
                print(f"    âœ… {package_name} is installed")
            except ImportError:
                print(f"    âŒ {package_name} is not installed")
                self.messages.append(f"Missing required dependency: {package_name}")
                all_required_installed = False
        
        # Check optional packages (for provider integration tests)
        print("\n  Optional packages (for provider integration):")
        optional_missing = []
        for import_name, package_name in optional_packages:
            try:
                __import__(import_name)
                print(f"    âœ… {package_name} is installed")
            except ImportError:
                print(f"    âš ï¸  {package_name} is not installed (needed for {import_name} provider tests)")
                optional_missing.append(package_name)
        
        if optional_missing:
            print(f"\n  â„¹ï¸  To test all providers, install: pip install {' '.join(optional_missing)}")
                
        self.results["dependencies"] = all_required_installed
        return all_required_installed
        
    def generate_report(self) -> str:
        """Generate a summary report of the verification."""
        report = ["\n" + "="*60]
        report.append("INTEGRATION TEST VERIFICATION REPORT")
        report.append("="*60)
        
        # Overall status
        all_passed = all(self.results.values())
        required_passed = self.results.get("files_exist", False) and self.results.get("dependencies", False)
        
        if all_passed:
            report.append("\nâœ… OVERALL STATUS: ALL CHECKS PASSED")
            report.append("\nYour integration test setup is properly configured!")
            report.append("\nTo run integration tests with real API calls:")
            report.append("  1. Install provider packages (if not already installed):")
            report.append("     pip install openai anthropic")
            report.append("  2. Set environment variables:")
            report.append("     export OPENAI_API_KEY='your-key-here'")
            report.append("     export ANTHROPIC_API_KEY='your-key-here'")
            report.append("  3. Start Ollama (if testing Ollama provider)")
            report.append("  4. Run: pytest -m integration tests/integration/")
        elif required_passed:
            report.append("\nâš ï¸  OVERALL STATUS: CORE SETUP OK, PROVIDER PACKAGES MISSING")
            report.append("\nYour integration test infrastructure is properly set up!")
            report.append("\nTo test specific providers, install their packages:")
            report.append("  â€¢ OpenAI: pip install openai")
            report.append("  â€¢ Anthropic: pip install anthropic")
            report.append("  â€¢ Ollama: Built-in (requires Ollama running locally)")
            report.append("\nThe test framework will automatically skip tests for missing providers.")
        else:
            report.append("\nâŒ OVERALL STATUS: SETUP ISSUES FOUND")
            report.append("\nCritical issues that need to be fixed:")
            for msg in self.messages:
                if "required" in msg.lower():
                    report.append(f"  â€¢ {msg}")
                
        # Detailed results
        report.append("\nðŸ“‹ Detailed Results:")
        for check, passed in self.results.items():
            status = "âœ… PASS" if passed else "âŒ FAIL"
            check_name = check.replace("_", " ").title()
            report.append(f"  â€¢ {check_name}: {status}")
            
        # Additional recommendations
        if not all_passed and required_passed:
            report.append("\nðŸ’¡ Recommendations:")
            report.append("  â€¢ The core test infrastructure is working correctly")
            report.append("  â€¢ Install provider packages to run their specific tests")
            report.append("  â€¢ Tests will automatically skip if API keys aren't set")
            
        report.append("\n" + "="*60)
        return "\n".join(report)
        
    def run(self) -> bool:
        """Run all verification checks."""
        print("ðŸš€ Starting integration test verification...\n")
        
        # Run all checks
        self.check_files_exist()
        self.check_dependencies()
        
        if self.results.get("files_exist", False):
            self.check_test_discovery()
            self.check_dry_run()
            
        # Generate and print report
        report = self.generate_report()
        print(report)
        
        # Return overall success
        return all(self.results.values())


def main():
    """Main entry point."""
    verifier = IntegrationTestVerifier()
    success = verifier.run()
    
    # Exit with appropriate code
    sys.exit(0 if success else 1)


if __name__ == "__main__":
    main()